##SLIDE 1###
Hello Everyone,
Before we Begin, I apology for my terrible accent, feel free to interupt me if i am not clear enought.

Well, my thesis subject is :
"Systemic simulation for the extraction of patient's biological signature".

##SLIDE 2###
This Subject came from a collaboration between the "Laboratory of Medical Information Processing" and the laboratory of B lymphocytes and Autoimmunity wich is taking part in the PRECISEADS project.

The thesis is co-directed by Alain Saraux and Pascal Redou, and we've got also Christophe Legal and Laurent Gaubert as supervisors.

##SLIDE 3###
The main idea behind this subject is to learn a model of a complex system, in this case a patient, from data generated by the PRECISADS project.

A very classic way to proceed would be to use a deep learing approcah, which is very powerful but introduce black boxes in the modelisation and that a thing we want to avoid because we want to have a clear understanding of the system we are modeling.

So we are going to use a rule based approcah for the modelisation of the system, and then try to extract biological signatures from it.

##SLIDE 4###
To illustrate this idea, i would lile to present a little exemple of how the all thing work. The idea is to create a simple program that learn how to make a diagnosis using the clinical data available from the PRECISADS project.

The classical approach used by clinician is to run a kind of decision tree with the symptoms, and eventually use a scoring table to determine the disease. Of course this is an approximation but we will stick with that for this example case

Our programm have only access to the clinical data, it must learn from them how to make a diagnosis, the first step is to select the relevant features in the data, then create rules from the observations and apply thus rules.

##SLIDE 5###
So, the first step for our program is to select relevant features. It perform this task by running an algorithm, called the Boruta algorithm on the input dataset. The Boruta algorithm works this way:

First, for each features in our dataset, we create a "shadow feature", which is simply a random permutation of the values from the original feature.

Then we run a random forest algorithm, which is a classifier who can rank features by using something called the z-score, basically a measurement of the feature's contribution to the classification given by the algorithm.

Finally, we look at the maximum importance value of all shadow features and use it as a threshold to determine if the feature is relevant or not.

##SLIDE 6###
So I ran this little program on the data and ask him to find relevant features for two diseases, the rhumatoid arthrisis and Jogren syndrom[FIXE].

The program selected 27 features for the RA and 12 for the SjS and we can see the top ranking features are autoantibody CCP2, arthrisis for the RA and classical Jogren symptoms, autoantibody for the SjS so everythong seems normal.

The really intersting part is that the program rejected 80 and 90 % of the features, which is a good thing: we have considerably reduce the dimmension of our problem.

##SLIDE 7### 
Now that we have a reduce set of feature we want to find frequent patterns among them.

A frequent pattern is an ensemble of items (E) [FIXE] ...

The classical approach to find frequent patterns is to generate all the possible combinaisons and then scan the dataset to test the support. It's not a really good idea, as you can see in this anmiation, there are really a lot of possibilities and that what we call a N(p) complex problem, which is not a great things for a computer to solve.

But there is an alternative, an algorithm to transform the input dataset into a frequent pattern tree structure that can be mined, I am not going to cover the details of the construction and mining process in this seminar but you can see the difference on this example, less patterns to examine, so you are basically saving a lot of computing power.

##SLIDE 8### RULES
[FIXEME]


##SLIDE 9### RULES RESULTS
Well, we can the here the result of the rules generation process
We've got this two exemple of rules, a very simple one for the RA case, with strong support and good confidence and one a bit more complex for SjS with lower confidence and not a really good support, seems that, as in real life, RA is easier to identify than SjS.

Just for the anecdote, while i was running my program for the firt time the rules i obtained where incredibly strong, work in any cases and where 100% accurate, then i realise that the dataset i used to train the program was containinng the diagnosis of patients, so basically the program just output rules like "If your patient is diagnosed with RA then the patient diagnosis must be RA".

Anyway, that was just an example of differents process involved in a rules based approach. Now what we want to do is to use this kind of approaches not on clinical data but on biological data from the PRECISADS project. 

##SLIDE 10### DISCRETIZATION - WHY ?
But first we are going to add a step: the discretization.
So why perform a discretization on our data ?
Well, many machine learning algorithm work only on discrete data, for exemple
the rule generation method we use need discrete data as input.
And Discrete data can also improve the efficiency of the algorithme, correct the skewed distribution, reduce the influence of outliers and sometimes can even behave as a feature selection method.

##SLIDE 11###
So my first attempt to discretize the data was to design this algorithm.
The idea is to perform a random discretization an the entire dataset and considere the discretize dataset as an "individual". We create multiple individual whith different discretization to form a population and then i basicaly run a genetic algorithm where 3 clasifiers act as a scoring function.

It was kind of greedy, I implement a multi-threading approach for the scoring part of the algorithm and ultimately let it run for a while.

Well, also a funny story here: first time a tested it, i let it run over a few days and i was pleased to found that the algorithm had learn a discretization that can lead to a score of 80% good classifications among patients (sick vs control). But then i remember that I have not yet implement a cross validation method for the classifier and that the dataset i was using contained about 20% control ...

Long story short, the algorithm learn that if he tag every patients as sick he had 8 chances out of 10 to give the correct answer ... 


##SLIDE12### The Ameva Algorithm
While this stuff was running i used my time to read a few articles on the subject and I found a few algorithm who seems to do a very good job, one of them is the Ameva algorithm.

[FIXEME]

##SLIDE13###
Well now that we have discretized data, here are the results of a selection features with the Boruta algorithm we have seen earlier. 

As you can see the analysis was perform on the 6 first panel, only on phase 1 patients.

for panel 1 we rejected only [FIXEME] % of the features,
[FIXEME], for panel 2
[FIXEME], for panel 3
[FIXEME], for panel 4
[FIXEME], for panel 5
and [FIXEME] for panel 6

##SLIDE 14### Results - again
To summerize, if we are looking at all the panels, we found [FIXEME] of rejected features ans [FIXEME] % of saved features.

Each of the features have a score according to its importance and it's possible to rank them as follow.

You see for exemple for panel 1 that this one is more relevant than this one ...
and so on for the orther panels.

##SLIDE 15###
Once we have selected the features that we thought are intersting we can go trough the rules generation process.

First, we try to find rules that can identify the diagnosis of a patient using only cytometry data, as you can see we generate a lot of rules, we applied a filter, make sure that the right term of the equation is a diagnosis and see the results.

And the results are not really good, as you can see we found rules with high confidence but a very low support, it's seem complicate to predict a diagnosis only with this kind of data.

##SLIDE 16###
But, if are looking for rules that does not necessaraly predict a diagnosis we found more interesting rules, with higher support and still strong confidence.

This rules could be intersing to understand the structure of the dataset.

##SLIDE 17###
Discretization is a key process in this workflow.
I have tried several algorithms and talk with my supervisors about a few improvements for this particular step.

The first thing we can do is to map the variable's values to a 0-1 interval and use this representation to have a probabilistic approach.

The second things we want to do is to use the process of rules extraction as a scoring method for the discretization: we want to kow if the discretization we used is at the origin at the apparition or disparition of an information, in this case: a rule.

##SLIDE 18###
We have also a few other things to investigate:
Implement an inference engine, I am probably going to do that over the summer
Use more data, not only flow cytometry stuff.
Inlcude patients from Inception phase.

##SLIDE 19###
thank for your time.

