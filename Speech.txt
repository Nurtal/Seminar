##SLIDE 1###
Hello Everyone,
Before we Begin, I apology for my terrible accent, feel free to interupt me if i am not clear enought.

Well, my thesis subject is :
"Systemic simulation for the extraction of patient's biological signature".

##SLIDE 2###
This Subject came from a collaboration between the "Laboratory of Medical Information Processing" and the laboratory of B lymphocytes and Autoimmunity wich is taking part in the PRECISEADS project.

The thesis is co-directed by Alain Saraux and Pascal Redou, and we've got also Christophe Legal and Laurent Gaubert as supervisors.

##SLIDE 3###
The main idea behind this subject is to learn a model of a complex system, in this case a patient, from data generated by the PRECISADS project.

A very classic way to proceed would be to use a deep learing approcah, which is very powerful but introduce black boxes in the modelisation and that a thing we want to avoid because we want to have a clear understanding of the system we are modeling.

So we are going to use a rule based approcah for the modelisation of the system, and then try to extract biological signatures from it.

##SLIDE 4###
To illustrate this idea, i would lile to present a little exemple of how the all thing work. The idea is to create a simple program that learn how to make a diagnosis using the clinical data available from the PRECISADS project.

The classical approach used by clinician is to run a kind of decision tree with the symptoms, and eventually use a scoring table to determine the disease. Of course this is an approximation but we will stick with that for this example case

Our programm have only access to the clinical data, it must learn from them how to make a diagnosis, the first step is to select the relevant features in the data, then create rules from the observations and apply thus rules.

##SLIDE 5###
So, the first step for our program is to select relevant features. It perform this task by running an algorithm, called the Boruta algorithm on the input dataset. The Boruta algorithm works this way:

First, for each features in our dataset, we create a "shadow feature", which is simply a random permutation of the values from the original feature.

Then we run a random forest algorithm, which is a classifier who can rank features by using something called the z-score, basically a measurement of the feature's contribution to the classification given by the algorithm.

Finally, we look at the maximum importance value of all shadow features and use it as a threshold to determine if the feature is relevant or not.

##SLIDE 6###
So I ran this little program on the data and ask him to find relevant features for two diseases, the rhumatoid arthrisis and Jogren syndrom[FIXE].

The program selected 27 features for the RA and 12 for the SjS and we can see the top ranking features are autoantibody CCP2, arthrisis for the RA and classical Jogren symptoms, autoantibody for the SjS so everythong seems normal.

The really intersting part is that the program rejected 80 and 90 % of the features, which is a good thing: we have considerably reduce the dimmension of our problem.

##SLIDE 7### 
Now that we have a reduce set of feature we want to find frequent patterns among them.

A frequent pattern is an ensemble of items (E) [FIXE] ...

The classical approach to find frequent patterns is to generate all the possible combinaisons and then scan the dataset to test the support. It's not a really good idea, as you can see in this anmiation, there are really a lot of possibilities and that what we call a N(p) complex problem, which is not a great things for a computer to solve.

But there is an alternative, an algorithm to transform the input dataset into a frequent pattern tree structure that can be mined, I am not going to cover the details of the construction and mining process in this seminar but you can see the difference on this example, less patterns to examine, so you are basically saving a lot of computing power.

##SLIDE 8### RULES
[FIXEME]


##SLIDE 9### RULES RESULTS
Well, we can the here the result of the rules generation process
We've got this two exemple of rules, a very simple one for the RA case, with strong support and good confidence and one a bit more complex for SjS with lower confidence and not a really good support, seems that, as in real life, RA is easier to identify than SjS.

Just for the anecdote, while i was running my program for the firt time the rules i obtained where incredibly strong, work in any cases and where 100% accurate, then i realise that the dataset i used to train the program was containinng the diagnosis of patients, so basically the program just output rules like "If your patient is diagnosed with RA then the patient diagnosis must be RA".

Anyway, that was just an example of differents process involved in a rules based approach. Now what we want to do is to use this kind of approaches not on clinical data but on biological data from the PRECISADS project. 

##SLIDE 10### DISCRETIZATION - WHY ?
But first we are going to add a step: the discretization.
So why perform a discretization on our data ?
Well, many machine learning algorithm work only on discrete data, for exemple
the rule generation method we use need discrete data as input.
And Discrete data can also improve the efficiency of the algorithme, correct the skewed distribution, reduce the influence of outliers and sometimes can even perform a kinf of feature selection.

##SLIDE 11###

